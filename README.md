# Multimodal-Adaptive-RAG
Multimodal Adaptive Retrieval Augmented Generation through Internal Representation Learning

# Multimodal Adaptive Retrieval-Augmented Generation (MM-ARAG)

This repository provides the implementation of **Multimodal Adaptive Retrieval-Augmented Generation through Internal Representation Learning**.  
The project investigates how **internal hidden representations** can be leveraged to decide whether retrieval is necessary in multimodal question answering (VQA) tasks.

We conduct experiments with **three different backbone models** across **three benchmark datasets**:
- **OK-VQA**
- **InfoSeek**
- **E-VQA**

---

## ðŸ“‚ Repository Structure
â”œâ”€â”€ classifier/ # Classifier training code (hidden state + image features â†’ prediction)
â”œâ”€â”€ okvqa/ # OK-VQA dataset preprocessing & evaluation scripts
â”œâ”€â”€ infoseek/ # InfoSeek dataset preprocessing & evaluation scripts
â”œâ”€â”€ evqa/ # E-VQA dataset preprocessing & evaluation scripts
â”œâ”€â”€ scripts/ # Bash scripts to reproduce experiments
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation
